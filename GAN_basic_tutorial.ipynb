{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433a1bfa-c491-45f5-8000-b71d231eb372",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see Bridge with NumPy). Tensors are also optimized for automatic differentiation (we’ll see more about that later in the Autograd section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c165407d-9aba-481c-95c3-5f925062defc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc1641-c3dd-42a1-8a54-de1750e59042",
   "metadata": {},
   "source": [
    "## Initializing a Tensor\n",
    "\n",
    "Tensors can be initialized in various ways. Take a look at the following examples:\n",
    "\n",
    "Directly from data\n",
    "\n",
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb75132a-218d-4f69-ab8c-26c14af5577e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "#tensors can run on GPUs or other hardware accelerators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75cc821-e4d9-4cbb-b36d-b305596eefa9",
   "metadata": {},
   "source": [
    "## From a NumPy array\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa - see Bridge with NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647dfb4d-4d6f-47a0-88c4-f5c50ea9e1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f0408-1980-4044-923b-647c0034507a",
   "metadata": {},
   "source": [
    "## From another tensor:\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c876e1c-0fab-4537-a98b-e587260a76ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.8494, 0.5638],\n",
      "        [0.8823, 0.5612]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd33113-d409-40e9-b8b9-4385745c6c3e",
   "metadata": {},
   "source": [
    "## With random or constant values:\n",
    "\n",
    "shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4fd2079-ec6d-40d0-9d1a-fb16590d518f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.5046, 0.4574, 0.9788],\n",
      "        [0.3400, 0.3422, 0.6501]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b0ac26-9e94-4414-93c0-a60443517e90",
   "metadata": {},
   "source": [
    "## Attributes of a Tensor\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d491eeb-b6a8-4dac-b55c-e03eadd1d795",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb141d-9b70-4932-b29f-f1602f190c5d",
   "metadata": {},
   "source": [
    "## Operations on Tensors\n",
    "\n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described here.\n",
    "\n",
    "Each of these operations can be run on the GPU (at typically higher speeds than on a CPU). If you’re using Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.\n",
    "\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc118b2c-f123-4ad6-898d-5e2239e492e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e1de1-497b-4dde-8a37-4acfec4bb03e",
   "metadata": {},
   "source": [
    "Try out some of the operations from the list. If you’re familiar with the NumPy API, you’ll find the Tensor API a breeze to use.\n",
    "\n",
    "Standard numpy-like indexing and slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9298aa45-b13c-4ad9-b0b9-2a4d1d8d18dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651503a3-efa2-4036-88b0-4079f8922ae5",
   "metadata": {},
   "source": [
    "## Joining tensors \n",
    "You can use torch.cat to concatenate a sequence of tensors along a given dimension. See also torch.stack, another tensor joining option that is subtly different from torch.cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6fcd102-0f30-4d2a-a5ab-dd92b9106370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9740d75d-5a82-4e39-a57e-2ec2d4eb44de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c639d-12e0-4ab0-ae3f-ddd4b5b18a16",
   "metadata": {},
   "source": [
    "## Single-element tensors\n",
    "If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364efca4-119c-4752-961f-7570714cec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.) <class 'torch.Tensor'>\n",
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg, type(agg))\n",
    "print(agg_item, type(agg_item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe788b-4eef-4d40-8bbb-f965eb07055e",
   "metadata": {},
   "source": [
    "## In-place operations \n",
    "Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91846518-b5c4-4ce9-872c-390943d99e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28725eaf-79d9-4276-8533-f4be3aa4b60e",
   "metadata": {},
   "source": [
    "## Bridge with NumPy\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.\n",
    "Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba9cfa9-c939-4b10-a728-c6f60bbf0675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd6613c3-b7ca-4219-b13b-fcefdea1d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6dfec4-51b8-4879-bbc9-c3222422cc04",
   "metadata": {},
   "source": [
    "## NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41db5b88-3167-46fa-8ab2-65d5a8b66eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208a4fe-3b10-4d5f-a300-9b65254f2ab8",
   "metadata": {},
   "source": [
    "## Changes in the NumPy array reflects in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cbcd966-b807-4d15-bdaa-4c3a3ff1f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28855fd4-6711-437c-b7bb-beabf7b7d04e",
   "metadata": {},
   "source": [
    "# Datasets & DataLoaders\n",
    "\n",
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass torch.utils.data.Dataset and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can find them here: Image Datasets, Text Datasets, and Audio Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1071d6-a6c0-43f2-aec7-83a7c3e613a3",
   "metadata": {},
   "source": [
    "## Loading a Dataset\n",
    "\n",
    "Here is an example of how to load the Fashion-MNIST dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.\n",
    "\n",
    "We load the FashionMNIST Dataset with the following parameters:\n",
    "\n",
    "        root is the path where the train/test data is stored,\n",
    "\n",
    "        train specifies training or test dataset,\n",
    "\n",
    "        download=True downloads the data from the internet if it’s not available at root.\n",
    "\n",
    "        transform and target_transform specify the feature and label transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a6156a-dc9e-4e49-a8b7-d5c6522d4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be480ba8-64ad-4c2a-bde0-6bfbe4b7f296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchaudio import datasets as ta\n",
    "mus_data_train = ta.MUSDB_HQ(\n",
    "    root=\"data\",\n",
    "    subset =\"train\",\n",
    "    download=True\n",
    ")\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "#     mus_data,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     num_workers=args.nThreads)\n",
    "\n",
    "\n",
    "\n",
    "mus_data_test = ta.MUSDB_HQ(\n",
    "    root=\"data\",\n",
    "    subset =\"test\",\n",
    "    download=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f649f9-8cf2-408c-b4c5-1f93053809a8",
   "metadata": {},
   "source": [
    "## Iterating and Visualizing the Dataset\n",
    "\n",
    "We can index Datasets manually like a list: training_data[index]. We use matplotlib to visualize some samples in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab71eee-d74e-48d5-a352-f72d3493c16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443df4db-5382-4ad2-9c22-1364a42dea47",
   "metadata": {},
   "source": [
    "## Creating a Custom Dataset for your files\n",
    "\n",
    "A custom Dataset class must implement three functions: __init__, __len__, and __getitem__. Take a look at this implementation; the FashionMNIST images are stored in a directory img_dir, and their labels are stored separately in a CSV file annotations_file.\n",
    "\n",
    "In the next sections, we’ll break down what’s happening in each of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87556cc-d340-4532-b4ad-61d552467e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38c748-637e-4712-8cbe-983e7ee85e90",
   "metadata": {},
   "source": [
    "## __init__\n",
    "\n",
    "The __init__ function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\n",
    "\n",
    "The labels.csv file looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14407a0-a3c8-437a-9d0d-c4b1fd5f8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tshirt1.jpg, 0\n",
    "# tshirt2.jpg, 0\n",
    "# #......\n",
    "# ankleboot999.jpg, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dbd1bc0-bc00-4eef-bbdc-d8078ea8dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56bc60-c7d1-4485-bf24-db1663cecb93",
   "metadata": {},
   "source": [
    "## __len__\n",
    "\n",
    "The __len__ function returns the number of samples in our dataset.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a992069a-5613-443e-bc21-08a8871c583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c9fb8-5c9e-4c5e-8cd0-dfffbb1dc7de",
   "metadata": {},
   "source": [
    "## __getitem__\n",
    "\n",
    "The __getitem__ function loads and returns a sample from the dataset at the given index idx. Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image, retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d080c32d-a97f-4f27-9bea-83dbeb5c65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = read_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a314ff0-90ae-4e46-a9c8-530481e62932",
   "metadata": {},
   "source": [
    "## Preparing your data for training with DataLoaders\n",
    "\n",
    "The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
    "\n",
    "DataLoader is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2916edcf-eba3-4075-b838-359ecb91a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acda9c0-c1ae-47bb-bf2e-8f1f26ebce10",
   "metadata": {},
   "source": [
    "## Iterate through the DataLoader\n",
    "\n",
    "We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4974c97b-849d-447f-ac58-05ecd2b4ffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c75c04-f0b3-475f-8d50-ac30c875f6b3",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training.\n",
    "\n",
    "All TorchVision datasets have two parameters -transform to modify the features and target_transform to modify the labels - that accept callables containing the transformation logic. The torchvision.transforms module offers several commonly-used transforms out of the box.\n",
    "\n",
    "The FashionMNIST features are in PIL Image format, and the labels are integers. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make these transformations, we use ToTensor and Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cc8fb1e-359c-4c4d-8a5c-3332dce7aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6b665-ccfc-4891-b0c0-453b5568d785",
   "metadata": {},
   "source": [
    "## ToTensor()\n",
    "\n",
    "ToTensor converts a PIL image or NumPy ndarray into a FloatTensor. and scales the image’s pixel intensity values in the range [0., 1.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8debf-91d8-4711-9ed8-efc6771acf95",
   "metadata": {},
   "source": [
    "## Lambda Transforms\n",
    "\n",
    "Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls scatter_ which assigns a value=1 on the index as given by the label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f54cfe0-86fc-4d91-afdf-199c16a2cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b137f-f0fc-4aff-8cf6-c136f73696de",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "\n",
    "Neural networks comprise of layers/modules that perform operations on data. The torch.nn namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the nn.Module. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n",
    "\n",
    "In the following sections, we’ll build a neural network to classify images in the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ba44c9-cc59-4d58-a620-e9e04fbb54a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc33972-b07a-4696-881a-484c0437f3bb",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "\n",
    "We want to be able to train our model on a hardware accelerator like the GPU, if it is available. Let’s check to see if torch.cuda is available, else we continue to use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f6ee44f-f368-4132-8971-9704782ba5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82897e6e-4c3a-4ccb-b274-66b288114792",
   "metadata": {},
   "source": [
    "## Define the Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa780b9-6021-46fd-8e88-99c4345b91d8",
   "metadata": {},
   "source": [
    "When defining our own neural networks, we use a subclass of nn.Module.\n",
    "We do this using super().\n",
    "It gives you access to methods in a superclass from the subclass that inherits from it and returns a temporary object of the superclass that then allows you to call that superclass’s methods. \n",
    "Why would you want to do any of this? \n",
    "While the possibilities are limited by your imagination, a common use case is building classes that extend the functionality of previously built classes. \n",
    " Calling the previously built methods with super() saves you from needing to rewrite those methods in your subclass, and allows you to swap out superclasses with minimal code changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a95032-02b3-42bd-ba36-8029bcc90fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rectangle:\n",
    "    def __init__(self, length, width):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "    def area(self):\n",
    "        return self.length * self.width\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * self.length + 2 * self.width\n",
    "\n",
    "# Here we declare that the Square class inherits from the Rectangle class\n",
    "class Square(Rectangle):\n",
    "    def __init__(self, length):\n",
    "        super().__init__(length, length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbcd447-fd87-420b-882c-0c4484834aee",
   "metadata": {},
   "source": [
    "Here, you’ve used super() to call the __init__() of the Rectangle class, allowing you to use it in the Square class without repeating code. Below, the core functionality remains after making changes\n",
    "Rectangle is the superclass, and Square is the subclass. \n",
    "Because the Square and Rectangle .__init__() methods are so similar, you can simply call the superclass’s .__init__() method (Rectangle.__init__()) from that of Square by using super(). This sets the .length and .width attributes even though you just had to supply a single length parameter to the Square constructor.\n",
    "\n",
    "When you run this, even though your Square class doesn’t explicitly implement it, the call to .area() will use the .area() method in the superclass and print 16. The Square class inherited .area() from the Rectangle class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8724ab-b739-4dad-8f45-78d7f01d8c99",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d51d4-efa2-4702-ba2d-c5fae1c75a4e",
   "metadata": {},
   "source": [
    "     \n",
    "\n",
    "Parameter\n",
    "\t\n",
    "\n",
    "A kind of Tensor that is to be considered a module parameter.\n",
    "\n",
    "UninitializedParameter\n",
    "\t\n",
    "\n",
    "A parameter that is not initialized.\n",
    "\n",
    "UninitializedBuffer\n",
    "\t\n",
    "\n",
    "A buffer that is not initialized.\n",
    "Containers\n",
    "\n",
    "Module\n",
    "\t\n",
    "\n",
    "Base class for all neural network modules.\n",
    "\n",
    "Sequential\n",
    "\t\n",
    "\n",
    "A sequential container.\n",
    "\n",
    "ModuleList\n",
    "\t\n",
    "\n",
    "Holds submodules in a list.\n",
    "\n",
    "ModuleDict\n",
    "\t\n",
    "\n",
    "Holds submodules in a dictionary.\n",
    "\n",
    "ParameterList\n",
    "\t\n",
    "\n",
    "Holds parameters in a list.\n",
    "\n",
    "ParameterDict\n",
    "\t\n",
    "\n",
    "Holds parameters in a dictionary.\n",
    "\n",
    "Global Hooks For Module\n",
    "\n",
    "register_module_forward_pre_hook\n",
    "\t\n",
    "\n",
    "Registers a forward pre-hook common to all modules.\n",
    "\n",
    "register_module_forward_hook\n",
    "\t\n",
    "\n",
    "Registers a global forward hook for all the modules\n",
    "\n",
    "register_module_backward_hook\n",
    "\t\n",
    "\n",
    "Registers a backward hook common to all the modules.\n",
    "\n",
    "register_module_full_backward_hook\n",
    "\t\n",
    "\n",
    "Registers a backward hook common to all the modules.\n",
    "Convolution Layers\n",
    "\n",
    "nn.Conv1d\n",
    "\t\n",
    "\n",
    "Applies a 1D convolution over an input signal composed of several input planes.\n",
    "\n",
    "nn.Conv2d\n",
    "\t\n",
    "\n",
    "Applies a 2D convolution over an input signal composed of several input planes.\n",
    "\n",
    "nn.Conv3d\n",
    "\t\n",
    "\n",
    "Applies a 3D convolution over an input signal composed of several input planes.\n",
    "\n",
    "nn.ConvTranspose1d\n",
    "\t\n",
    "\n",
    "Applies a 1D transposed convolution operator over an input image composed of several input planes.\n",
    "\n",
    "nn.ConvTranspose2d\n",
    "\t\n",
    "\n",
    "Applies a 2D transposed convolution operator over an input image composed of several input planes.\n",
    "\n",
    "nn.ConvTranspose3d\n",
    "\t\n",
    "\n",
    "Applies a 3D transposed convolution operator over an input image composed of several input planes.\n",
    "\n",
    "nn.LazyConv1d\n",
    "\t\n",
    "\n",
    "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LazyConv2d\n",
    "\t\n",
    "\n",
    "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LazyConv3d\n",
    "\t\n",
    "\n",
    "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LazyConvTranspose1d\n",
    "\t\n",
    "\n",
    "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LazyConvTranspose2d\n",
    "\t\n",
    "\n",
    "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LazyConvTranspose3d\n",
    "\t\n",
    "\n",
    "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size(1).\n",
    "\n",
    "nn.Unfold\n",
    "\t\n",
    "\n",
    "Extracts sliding local blocks from a batched input tensor.\n",
    "\n",
    "nn.Fold\n",
    "\t\n",
    "\n",
    "Combines an array of sliding local blocks into a large containing tensor.\n",
    "Pooling layers\n",
    "\n",
    "nn.MaxPool1d\n",
    "\t\n",
    "\n",
    "Applies a 1D max pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.MaxPool2d\n",
    "\t\n",
    "\n",
    "Applies a 2D max pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.MaxPool3d\n",
    "\t\n",
    "\n",
    "Applies a 3D max pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.MaxUnpool1d\n",
    "\t\n",
    "\n",
    "Computes a partial inverse of MaxPool1d.\n",
    "\n",
    "nn.MaxUnpool2d\n",
    "\t\n",
    "\n",
    "Computes a partial inverse of MaxPool2d.\n",
    "\n",
    "nn.MaxUnpool3d\n",
    "\t\n",
    "\n",
    "Computes a partial inverse of MaxPool3d.\n",
    "\n",
    "nn.AvgPool1d\n",
    "\t\n",
    "\n",
    "Applies a 1D average pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.AvgPool2d\n",
    "\t\n",
    "\n",
    "Applies a 2D average pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.AvgPool3d\n",
    "\t\n",
    "\n",
    "Applies a 3D average pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.FractionalMaxPool2d\n",
    "\t\n",
    "\n",
    "Applies a 2D fractional max pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.FractionalMaxPool3d\n",
    "\t\n",
    "\n",
    "Applies a 3D fractional max pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.LPPool1d\n",
    "\t\n",
    "\n",
    "Applies a 1D power-average pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.LPPool2d\n",
    "\t\n",
    "\n",
    "Applies a 2D power-average pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.AdaptiveMaxPool1d\n",
    "\t\n",
    "\n",
    "Applies a 1D adaptive max pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.AdaptiveMaxPool2d\n",
    "\t\n",
    "\n",
    "Applies a 2D adaptive max pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.AdaptiveMaxPool3d\n",
    "\t\n",
    "\n",
    "Applies a 3D adaptive max pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.AdaptiveAvgPool1d\n",
    "\t\n",
    "\n",
    "Applies a 1D adaptive average pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.AdaptiveAvgPool2d\n",
    "\t\n",
    "\n",
    "Applies a 2D adaptive average pooling over an input signal composed of several input planes.\n",
    "\n",
    "nn.AdaptiveAvgPool3d\n",
    "\t\n",
    "\n",
    "Applies a 3D adaptive average pooling over an input signal composed of several input planes.\n",
    "Padding Layers\n",
    "\n",
    "nn.ReflectionPad1d\n",
    "\t\n",
    "\n",
    "Pads the input tensor using the reflection of the input boundary.\n",
    "\n",
    "nn.ReflectionPad2d\n",
    "\t\n",
    "\n",
    "Pads the input tensor using the reflection of the input boundary.\n",
    "\n",
    "nn.ReflectionPad3d\n",
    "\t\n",
    "\n",
    "Pads the input tensor using the reflection of the input boundary.\n",
    "\n",
    "nn.ReplicationPad1d\n",
    "\t\n",
    "\n",
    "Pads the input tensor using replication of the input boundary.\n",
    "\n",
    "nn.ReplicationPad2d\n",
    "\t\n",
    "\n",
    "Pads the input tensor using replication of the input boundary.\n",
    "\n",
    "nn.ReplicationPad3d\n",
    "\t\n",
    "\n",
    "Pads the input tensor using replication of the input boundary.\n",
    "\n",
    "nn.ZeroPad2d\n",
    "\t\n",
    "\n",
    "Pads the input tensor boundaries with zero.\n",
    "\n",
    "nn.ConstantPad1d\n",
    "\t\n",
    "\n",
    "Pads the input tensor boundaries with a constant value.\n",
    "\n",
    "nn.ConstantPad2d\n",
    "\t\n",
    "\n",
    "Pads the input tensor boundaries with a constant value.\n",
    "\n",
    "nn.ConstantPad3d\n",
    "\t\n",
    "\n",
    "Pads the input tensor boundaries with a constant value.\n",
    "Non-linear Activations (weighted sum, nonlinearity)\n",
    "\n",
    "nn.ELU\n",
    "\t\n",
    "\n",
    "Applies the Exponential Linear Unit (ELU) function, element-wise, as described in the paper: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).\n",
    "\n",
    "nn.Hardshrink\n",
    "\t\n",
    "\n",
    "Applies the Hard Shrinkage (Hardshrink) function element-wise.\n",
    "\n",
    "nn.Hardsigmoid\n",
    "\t\n",
    "\n",
    "Applies the Hardsigmoid function element-wise.\n",
    "\n",
    "nn.Hardtanh\n",
    "\t\n",
    "\n",
    "Applies the HardTanh function element-wise.\n",
    "\n",
    "nn.Hardswish\n",
    "\t\n",
    "\n",
    "Applies the Hardswish function, element-wise, as described in the paper: Searching for MobileNetV3.\n",
    "\n",
    "nn.LeakyReLU\n",
    "\t\n",
    "\n",
    "Applies the element-wise function:\n",
    "\n",
    "nn.LogSigmoid\n",
    "\t\n",
    "\n",
    "Applies the element-wise function:\n",
    "\n",
    "nn.MultiheadAttention\n",
    "\t\n",
    "\n",
    "Allows the model to jointly attend to information from different representation subspaces as described in the paper: Attention Is All You Need.\n",
    "\n",
    "nn.PReLU\n",
    "\t\n",
    "\n",
    "Applies the element-wise function:\n",
    "\n",
    "nn.ReLU\n",
    "\t\n",
    "\n",
    "Applies the rectified linear unit function element-wise:\n",
    "\n",
    "nn.ReLU6\n",
    "\t\n",
    "\n",
    "Applies the element-wise function:\n",
    "\n",
    "nn.RReLU\n",
    "\t\n",
    "\n",
    "Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:\n",
    "\n",
    "nn.SELU\n",
    "\t\n",
    "\n",
    "Applied element-wise, as:\n",
    "\n",
    "nn.CELU\n",
    "\t\n",
    "\n",
    "Applies the element-wise function:\n",
    "\n",
    "nn.GELU\n",
    "\t\n",
    "\n",
    "Applies the Gaussian Error Linear Units function:\n",
    "\n",
    "nn.Sigmoid\n",
    "\t\n",
    "\n",
    "Applies the element-wise function:\n",
    "\n",
    "nn.SiLU\n",
    "\t\n",
    "\n",
    "Applies the Sigmoid Linear Unit (SiLU) function, element-wise.\n",
    "\n",
    "nn.Mish\n",
    "\t\n",
    "\n",
    "Applies the Mish function, element-wise.\n",
    "\n",
    "nn.Softplus\n",
    "\t\n",
    "\n",
    "Applies the Softplus function Softplus(x)=1β∗log⁡(1+exp⁡(β∗x))Softplus(x)=β1​∗log(1+exp(β∗x)) element-wise.\n",
    "\n",
    "nn.Softshrink\n",
    "\t\n",
    "\n",
    "Applies the soft shrinkage function elementwise:\n",
    "\n",
    "nn.Softsign\n",
    "\t\n",
    "\n",
    "Applies the element-wise function:\n",
    "\n",
    "nn.Tanh\n",
    "\t\n",
    "\n",
    "Applies the Hyperbolic Tangent (Tanh) function element-wise.\n",
    "\n",
    "nn.Tanhshrink\n",
    "\t\n",
    "\n",
    "Applies the element-wise function:\n",
    "\n",
    "nn.Threshold\n",
    "\t\n",
    "\n",
    "Thresholds each element of the input Tensor.\n",
    "\n",
    "nn.GLU\n",
    "\t\n",
    "\n",
    "Applies the gated linear unit function GLU(a,b)=a⊗σ(b)GLU(a,b)=a⊗σ(b) where aa is the first half of the input matrices and bb is the second half.\n",
    "Non-linear Activations (other)\n",
    "\n",
    "nn.Softmin\n",
    "\t\n",
    "\n",
    "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1.\n",
    "\n",
    "nn.Softmax\n",
    "\t\n",
    "\n",
    "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.\n",
    "\n",
    "nn.Softmax2d\n",
    "\t\n",
    "\n",
    "Applies SoftMax over features to each spatial location.\n",
    "\n",
    "nn.LogSoftmax\n",
    "\t\n",
    "\n",
    "Applies the log⁡(Softmax(x))log(Softmax(x)) function to an n-dimensional input Tensor.\n",
    "\n",
    "nn.AdaptiveLogSoftmaxWithLoss\n",
    "\t\n",
    "\n",
    "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou.\n",
    "Normalization Layers\n",
    "\n",
    "nn.BatchNorm1d\n",
    "\t\n",
    "\n",
    "Applies Batch Normalization over a 2D or 3D input as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n",
    "\n",
    "nn.BatchNorm2d\n",
    "\t\n",
    "\n",
    "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n",
    "\n",
    "nn.BatchNorm3d\n",
    "\t\n",
    "\n",
    "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n",
    "\n",
    "nn.LazyBatchNorm1d\n",
    "\t\n",
    "\n",
    "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LazyBatchNorm2d\n",
    "\t\n",
    "\n",
    "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LazyBatchNorm3d\n",
    "\t\n",
    "\n",
    "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size(1).\n",
    "\n",
    "nn.GroupNorm\n",
    "\t\n",
    "\n",
    "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization\n",
    "\n",
    "nn.SyncBatchNorm\n",
    "\t\n",
    "\n",
    "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n",
    "\n",
    "nn.InstanceNorm1d\n",
    "\t\n",
    "\n",
    "Applies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.\n",
    "\n",
    "nn.InstanceNorm2d\n",
    "\t\n",
    "\n",
    "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.\n",
    "\n",
    "nn.InstanceNorm3d\n",
    "\t\n",
    "\n",
    "Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.\n",
    "\n",
    "nn.LazyInstanceNorm1d\n",
    "\t\n",
    "\n",
    "A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument of the InstanceNorm1d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LazyInstanceNorm2d\n",
    "\t\n",
    "\n",
    "A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument of the InstanceNorm2d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LazyInstanceNorm3d\n",
    "\t\n",
    "\n",
    "A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument of the InstanceNorm3d that is inferred from the input.size(1).\n",
    "\n",
    "nn.LayerNorm\n",
    "\t\n",
    "\n",
    "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization\n",
    "\n",
    "nn.LocalResponseNorm\n",
    "\t\n",
    "\n",
    "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.\n",
    "Recurrent Layers\n",
    "\n",
    "nn.RNNBase\n",
    "\t\n",
    "\n",
    "nn.RNN\n",
    "\t\n",
    "\n",
    "Applies a multi-layer Elman RNN with tanh⁡tanh or ReLUReLU non-linearity to an input sequence.\n",
    "\n",
    "nn.LSTM\n",
    "\t\n",
    "\n",
    "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n",
    "\n",
    "nn.GRU\n",
    "\t\n",
    "\n",
    "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n",
    "\n",
    "nn.RNNCell\n",
    "\t\n",
    "\n",
    "An Elman RNN cell with tanh or ReLU non-linearity.\n",
    "\n",
    "nn.LSTMCell\n",
    "\t\n",
    "\n",
    "A long short-term memory (LSTM) cell.\n",
    "\n",
    "nn.GRUCell\n",
    "\t\n",
    "\n",
    "A gated recurrent unit (GRU) cell\n",
    "Transformer Layers\n",
    "\n",
    "nn.Transformer\n",
    "\t\n",
    "\n",
    "A transformer model.\n",
    "\n",
    "nn.TransformerEncoder\n",
    "\t\n",
    "\n",
    "TransformerEncoder is a stack of N encoder layers.\n",
    "\n",
    "nn.TransformerDecoder\n",
    "\t\n",
    "\n",
    "TransformerDecoder is a stack of N decoder layers\n",
    "\n",
    "nn.TransformerEncoderLayer\n",
    "\t\n",
    "\n",
    "TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "\n",
    "nn.TransformerDecoderLayer\n",
    "\t\n",
    "\n",
    "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
    "Linear Layers\n",
    "\n",
    "nn.Identity\n",
    "\t\n",
    "\n",
    "A placeholder identity operator that is argument-insensitive.\n",
    "\n",
    "nn.Linear\n",
    "\t\n",
    "\n",
    "Applies a linear transformation to the incoming data: y=xAT+by=xAT+b\n",
    "\n",
    "nn.Bilinear\n",
    "\t\n",
    "\n",
    "Applies a bilinear transformation to the incoming data: y=x1TAx2+by=x1T​Ax2​+b\n",
    "\n",
    "nn.LazyLinear\n",
    "\t\n",
    "\n",
    "A torch.nn.Linear module where in_features is inferred.\n",
    "Dropout Layers\n",
    "\n",
    "nn.Dropout\n",
    "\t\n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.\n",
    "\n",
    "nn.Dropout1d\n",
    "\t\n",
    "\n",
    "Randomly zero out entire channels (a channel is a 1D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 1D tensor input[i,j]input[i,j]).\n",
    "\n",
    "nn.Dropout2d\n",
    "\t\n",
    "\n",
    "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 2D tensor input[i,j]input[i,j]).\n",
    "\n",
    "nn.Dropout3d\n",
    "\t\n",
    "\n",
    "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 3D tensor input[i,j]input[i,j]).\n",
    "\n",
    "nn.AlphaDropout\n",
    "\t\n",
    "\n",
    "Applies Alpha Dropout over the input.\n",
    "\n",
    "nn.FeatureAlphaDropout\n",
    "\t\n",
    "\n",
    "Randomly masks out entire channels (a channel is a feature map, e.g.\n",
    "Sparse Layers\n",
    "\n",
    "nn.Embedding\n",
    "\t\n",
    "\n",
    "A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "\n",
    "nn.EmbeddingBag\n",
    "\t\n",
    "\n",
    "Computes sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.\n",
    "Distance Functions\n",
    "\n",
    "nn.CosineSimilarity\n",
    "\t\n",
    "\n",
    "Returns cosine similarity between x1x1​ and x2x2​, computed along dim.\n",
    "\n",
    "nn.PairwiseDistance\n",
    "\t\n",
    "\n",
    "Computes the pairwise distance between input vectors, or between columns of input matrices.\n",
    "Loss Functions\n",
    "\n",
    "nn.L1Loss\n",
    "\t\n",
    "\n",
    "Creates a criterion that measures the mean absolute error (MAE) between each element in the input xx and target yy.\n",
    "\n",
    "nn.MSELoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xx and target yy.\n",
    "\n",
    "nn.CrossEntropyLoss\n",
    "\t\n",
    "\n",
    "This criterion computes the cross entropy loss between input logits and target.\n",
    "\n",
    "nn.CTCLoss\n",
    "\t\n",
    "\n",
    "The Connectionist Temporal Classification loss.\n",
    "\n",
    "nn.NLLLoss\n",
    "\t\n",
    "\n",
    "The negative log likelihood loss.\n",
    "\n",
    "nn.PoissonNLLLoss\n",
    "\t\n",
    "\n",
    "Negative log likelihood loss with Poisson distribution of target.\n",
    "\n",
    "nn.GaussianNLLLoss\n",
    "\t\n",
    "\n",
    "Gaussian negative log likelihood loss.\n",
    "\n",
    "nn.KLDivLoss\n",
    "\t\n",
    "\n",
    "The Kullback-Leibler divergence loss.\n",
    "\n",
    "nn.BCELoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:\n",
    "\n",
    "nn.BCEWithLogitsLoss\n",
    "\t\n",
    "\n",
    "This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "\n",
    "nn.MarginRankingLoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that measures the loss given inputs x1x1, x2x2, two 1D mini-batch or 0D Tensors, and a label 1D mini-batch or 0D Tensor yy (containing 1 or -1).\n",
    "\n",
    "nn.HingeEmbeddingLoss\n",
    "\t\n",
    "\n",
    "Measures the loss given an input tensor xx and a labels tensor yy (containing 1 or -1).\n",
    "\n",
    "nn.MultiLabelMarginLoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xx (a 2D mini-batch Tensor) and output yy (which is a 2D Tensor of target class indices).\n",
    "\n",
    "nn.HuberLoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.\n",
    "\n",
    "nn.SmoothL1Loss\n",
    "\t\n",
    "\n",
    "Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.\n",
    "\n",
    "nn.SoftMarginLoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xx and target tensor yy (containing 1 or -1).\n",
    "\n",
    "nn.MultiLabelSoftMarginLoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xx and target yy of size (N,C)(N,C).\n",
    "\n",
    "nn.CosineEmbeddingLoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that measures the loss given input tensors x1x1​, x2x2​ and a Tensor label yy with values 1 or -1.\n",
    "\n",
    "nn.MultiMarginLoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xx (a 2D mini-batch Tensor) and output yy (which is a 1D tensor of target class indices, 0≤y≤x.size(1)−10≤y≤x.size(1)−1):\n",
    "\n",
    "nn.TripletMarginLoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that measures the triplet loss given an input tensors x1x1, x2x2, x3x3 and a margin with a value greater than 00.\n",
    "\n",
    "nn.TripletMarginWithDistanceLoss\n",
    "\t\n",
    "\n",
    "Creates a criterion that measures the triplet loss given input tensors aa, pp, and nn (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n",
    "Vision Layers\n",
    "\n",
    "nn.PixelShuffle\n",
    "\t\n",
    "\n",
    "Rearranges elements in a tensor of shape (∗,C×r2,H,W)(∗,C×r2,H,W) to a tensor of shape (∗,C,H×r,W×r)(∗,C,H×r,W×r), where r is an upscale factor.\n",
    "\n",
    "nn.PixelUnshuffle\n",
    "\t\n",
    "\n",
    "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (∗,C,H×r,W×r)(∗,C,H×r,W×r) to a tensor of shape (∗,C×r2,H,W)(∗,C×r2,H,W), where r is a downscale factor.\n",
    "\n",
    "nn.Upsample\n",
    "\t\n",
    "\n",
    "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.\n",
    "\n",
    "nn.UpsamplingNearest2d\n",
    "\t\n",
    "\n",
    "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.\n",
    "\n",
    "nn.UpsamplingBilinear2d\n",
    "\t\n",
    "\n",
    "Applies a 2D bilinear upsampling to an input signal composed of several input channels.\n",
    "Shuffle Layers\n",
    "\n",
    "nn.ChannelShuffle\n",
    "\t\n",
    "\n",
    "Divide the channels in a tensor of shape (∗,C,H,W)(∗,C,H,W) into g groups and rearrange them as (∗,Cg,g,H,W)(∗,C,g​g,H,W), while keeping the original tensor shape.\n",
    "DataParallel Layers (multi-GPU, distributed)\n",
    "\n",
    "nn.DataParallel\n",
    "\t\n",
    "\n",
    "Implements data parallelism at the module level.\n",
    "\n",
    "nn.parallel.DistributedDataParallel\n",
    "\t\n",
    "\n",
    "Implements distributed data parallelism that is based on torch.distributed package at the module level.\n",
    "Utilities\n",
    "\n",
    "From the torch.nn.utils module\n",
    "\n",
    "clip_grad_norm_\n",
    "\t\n",
    "\n",
    "Clips gradient norm of an iterable of parameters.\n",
    "\n",
    "clip_grad_value_\n",
    "\t\n",
    "\n",
    "Clips gradient of an iterable of parameters at specified value.\n",
    "\n",
    "parameters_to_vector\n",
    "\t\n",
    "\n",
    "Convert parameters to one vector\n",
    "\n",
    "vector_to_parameters\n",
    "\t\n",
    "\n",
    "Convert one vector to the parameters\n",
    "\n",
    "prune.BasePruningMethod\n",
    "\t\n",
    "\n",
    "Abstract base class for creation of new pruning techniques.\n",
    "\n",
    "prune.PruningContainer\n",
    "\t\n",
    "\n",
    "Container holding a sequence of pruning methods for iterative pruning.\n",
    "\n",
    "prune.Identity\n",
    "\t\n",
    "\n",
    "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.\n",
    "\n",
    "prune.RandomUnstructured\n",
    "\t\n",
    "\n",
    "Prune (currently unpruned) units in a tensor at random.\n",
    "\n",
    "prune.L1Unstructured\n",
    "\t\n",
    "\n",
    "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.\n",
    "\n",
    "prune.RandomStructured\n",
    "\t\n",
    "\n",
    "Prune entire (currently unpruned) channels in a tensor at random.\n",
    "\n",
    "prune.LnStructured\n",
    "\t\n",
    "\n",
    "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.\n",
    "\n",
    "prune.CustomFromMask\n",
    "\t\n",
    "\n",
    "prune.identity\n",
    "\t\n",
    "\n",
    "Applies pruning reparametrization to the tensor corresponding to the parameter called name in module without actually pruning any units.\n",
    "\n",
    "prune.random_unstructured\n",
    "\t\n",
    "\n",
    "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random.\n",
    "\n",
    "prune.l1_unstructured\n",
    "\t\n",
    "\n",
    "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm.\n",
    "\n",
    "prune.random_structured\n",
    "\t\n",
    "\n",
    "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random.\n",
    "\n",
    "prune.ln_structured\n",
    "\t\n",
    "\n",
    "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest Ln-norm.\n",
    "\n",
    "prune.global_unstructured\n",
    "\t\n",
    "\n",
    "Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method.\n",
    "\n",
    "prune.custom_from_mask\n",
    "\t\n",
    "\n",
    "Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask.\n",
    "\n",
    "prune.remove\n",
    "\t\n",
    "\n",
    "Removes the pruning reparameterization from a module and the pruning method from the forward hook.\n",
    "\n",
    "prune.is_pruned\n",
    "\t\n",
    "\n",
    "Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.\n",
    "\n",
    "weight_norm\n",
    "\t\n",
    "\n",
    "Applies weight normalization to a parameter in the given module.\n",
    "\n",
    "remove_weight_norm\n",
    "\t\n",
    "\n",
    "Removes the weight normalization reparameterization from a module.\n",
    "\n",
    "spectral_norm\n",
    "\t\n",
    "\n",
    "Applies spectral normalization to a parameter in the given module.\n",
    "\n",
    "remove_spectral_norm\n",
    "\t\n",
    "\n",
    "Removes the spectral normalization reparameterization from a module.\n",
    "\n",
    "skip_init\n",
    "\t\n",
    "\n",
    "Given a module class object and args / kwargs, instantiates the module without initializing parameters / buffers.\n",
    "\n",
    "Parametrizations implemented using the new parametrization functionality in torch.nn.utils.parameterize.register_parametrization().\n",
    "\n",
    "parametrizations.orthogonal\n",
    "\t\n",
    "\n",
    "Applies an orthogonal or unitary parametrization to a matrix or a batch of matrices.\n",
    "\n",
    "parametrizations.spectral_norm\n",
    "\t\n",
    "\n",
    "Applies spectral normalization to a parameter in the given module.\n",
    "\n",
    "Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations.\n",
    "\n",
    "parametrize.register_parametrization\n",
    "\t\n",
    "\n",
    "Adds a parametrization to a tensor in a module.\n",
    "\n",
    "parametrize.remove_parametrizations\n",
    "\t\n",
    "\n",
    "Removes the parametrizations on a tensor in a module.\n",
    "\n",
    "parametrize.cached\n",
    "\t\n",
    "\n",
    "Context manager that enables the caching system within parametrizations registered with register_parametrization().\n",
    "\n",
    "parametrize.is_parametrized\n",
    "\t\n",
    "\n",
    "Returns True if module has an active parametrization.\n",
    "\n",
    "parametrize.ParametrizationList\n",
    "\t\n",
    "\n",
    "A sequential container that holds and manages the original or original0, original1, .\n",
    "\n",
    "Utility functions to calls a given Module in a stateless manner.\n",
    "\n",
    "stateless.functional_call\n",
    "\t\n",
    "\n",
    "Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.\n",
    "\n",
    "Utility functions in other modules\n",
    "\n",
    "nn.utils.rnn.PackedSequence\n",
    "\t\n",
    "\n",
    "Holds the data and list of batch_sizes of a packed sequence.\n",
    "\n",
    "nn.utils.rnn.pack_padded_sequence\n",
    "\t\n",
    "\n",
    "Packs a Tensor containing padded sequences of variable length.\n",
    "\n",
    "nn.utils.rnn.pad_packed_sequence\n",
    "\t\n",
    "\n",
    "Pads a packed batch of variable length sequences.\n",
    "\n",
    "nn.utils.rnn.pad_sequence\n",
    "\t\n",
    "\n",
    "Pad a list of variable length Tensors with padding_value\n",
    "\n",
    "nn.utils.rnn.pack_sequence\n",
    "\t\n",
    "\n",
    "Packs a list of variable length Tensors\n",
    "\n",
    "nn.Flatten\n",
    "\t\n",
    "\n",
    "Flattens a contiguous range of dims into a tensor.\n",
    "\n",
    "nn.Unflatten\n",
    "\t\n",
    "\n",
    "Unflattens a tensor dim expanding it to a desired shape.\n",
    "Quantized Functions\n",
    "\n",
    "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation.\n",
    "Lazy Modules Initialization\n",
    "\n",
    "nn.modules.lazy.LazyModuleMixin\n",
    "\t\n",
    "\n",
    "A mixin for modules that lazily initialize parameters, also known as \"lazy modules.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08303a64-50e2-4ab8-8e78-6ee5a283ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c86883-1947-4722-ba0f-c401009e0650",
   "metadata": {},
   "source": [
    "We create an instance of NeuralNetwork, and move it to the device, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "76127259-3fab-4511-94bf-d614f1c8ab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5402a2c-c556-4a20-a0c6-843605df934a",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call model.forward() directly!\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the nn.Softmax module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e881d8f-da0b-4d4d-ac74-91443d6d9441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:tensor([[[2.2326e-01, 6.3550e-01, 9.6674e-02, 7.4569e-01, 7.8951e-01,\n",
      "          3.5731e-01, 4.0480e-01, 5.3172e-01, 6.6741e-01, 3.6322e-01,\n",
      "          6.1539e-01, 2.4584e-01, 1.6337e-01, 8.0372e-01, 8.6587e-01,\n",
      "          8.4573e-01, 5.6141e-01, 8.8362e-01, 4.7098e-01, 2.3188e-01,\n",
      "          7.8699e-01, 6.3519e-01, 6.9737e-01, 8.7549e-01, 6.0278e-02,\n",
      "          1.9411e-01, 3.2337e-01, 2.5847e-02],\n",
      "         [8.6599e-01, 3.4186e-01, 8.1239e-01, 5.5617e-01, 6.2495e-01,\n",
      "          3.2503e-01, 6.7748e-01, 8.3886e-01, 7.6096e-02, 6.8888e-01,\n",
      "          8.2766e-01, 3.1909e-01, 4.8283e-02, 7.9986e-01, 4.1982e-01,\n",
      "          7.6125e-02, 5.2485e-01, 1.8458e-01, 9.1958e-01, 9.1773e-01,\n",
      "          7.1909e-01, 7.4993e-01, 2.9502e-01, 3.1089e-01, 8.8744e-01,\n",
      "          6.5912e-01, 2.2722e-01, 7.0294e-01],\n",
      "         [5.4573e-01, 9.6699e-01, 6.6759e-01, 5.8179e-01, 8.5798e-01,\n",
      "          9.6864e-01, 9.4937e-01, 6.2789e-01, 2.7613e-01, 6.8912e-02,\n",
      "          3.0666e-01, 1.1768e-01, 7.9823e-01, 2.9865e-01, 2.6199e-01,\n",
      "          8.8535e-01, 6.9001e-01, 2.5491e-02, 4.6172e-01, 6.3674e-01,\n",
      "          5.5763e-01, 6.5277e-01, 3.5251e-01, 2.5409e-01, 7.8695e-01,\n",
      "          3.8313e-01, 1.3719e-01, 6.9785e-01],\n",
      "         [2.5950e-02, 3.2768e-01, 4.9002e-01, 2.0939e-01, 8.0880e-01,\n",
      "          4.7645e-01, 5.7664e-01, 4.8348e-01, 2.2829e-01, 9.3296e-02,\n",
      "          8.5418e-02, 1.3398e-01, 2.5317e-01, 5.5360e-01, 2.0473e-01,\n",
      "          8.2307e-01, 6.4713e-01, 3.8851e-01, 4.7924e-01, 7.7068e-02,\n",
      "          3.6023e-02, 3.5986e-01, 7.5747e-01, 3.7016e-01, 1.2533e-01,\n",
      "          1.9555e-01, 1.1601e-01, 2.4085e-01],\n",
      "         [9.8149e-01, 2.7277e-01, 3.7964e-01, 9.3706e-01, 9.7307e-01,\n",
      "          9.4345e-01, 7.1149e-01, 2.5391e-01, 6.6932e-01, 9.9273e-01,\n",
      "          6.2613e-01, 3.3742e-01, 6.8614e-02, 8.7677e-01, 6.1758e-02,\n",
      "          7.5745e-01, 7.9491e-01, 5.1348e-01, 3.2584e-01, 9.3528e-01,\n",
      "          4.1427e-01, 9.8871e-01, 3.7849e-02, 3.1824e-01, 2.4361e-01,\n",
      "          4.5452e-01, 7.6345e-01, 3.6927e-01],\n",
      "         [5.0290e-01, 6.1490e-01, 3.4689e-02, 5.6448e-01, 1.2105e-01,\n",
      "          4.3244e-01, 8.5553e-01, 3.3452e-01, 9.2631e-01, 6.7197e-01,\n",
      "          1.0789e-01, 3.9762e-01, 3.1324e-01, 3.0621e-02, 2.4547e-01,\n",
      "          7.1301e-02, 8.4180e-01, 7.2446e-01, 6.5752e-02, 1.2433e-01,\n",
      "          4.1922e-01, 7.6648e-01, 2.3838e-01, 7.4441e-01, 9.5356e-01,\n",
      "          8.9647e-01, 8.8483e-04, 5.1068e-01],\n",
      "         [4.6471e-01, 4.9613e-01, 8.6062e-01, 2.3958e-02, 4.0231e-01,\n",
      "          2.3462e-01, 8.8028e-01, 5.8039e-01, 3.0959e-01, 1.0399e-01,\n",
      "          8.9413e-01, 3.9661e-01, 3.3447e-01, 4.0169e-01, 3.0137e-01,\n",
      "          9.6905e-01, 6.0523e-02, 4.8153e-01, 9.2523e-01, 8.8823e-01,\n",
      "          6.1205e-01, 9.8762e-01, 9.5537e-02, 2.2595e-01, 4.8435e-01,\n",
      "          1.5587e-01, 4.4650e-01, 3.9652e-01],\n",
      "         [3.9763e-01, 8.4113e-01, 8.4893e-01, 1.3503e-01, 6.1023e-01,\n",
      "          6.1785e-01, 9.6098e-01, 4.8342e-01, 5.4949e-02, 6.4840e-02,\n",
      "          8.9226e-01, 6.9268e-01, 9.0237e-01, 7.7606e-01, 4.0728e-01,\n",
      "          2.9865e-02, 9.4925e-01, 8.4503e-01, 3.4860e-01, 8.3268e-01,\n",
      "          2.5130e-01, 9.6964e-01, 1.3477e-01, 7.0057e-01, 4.3493e-01,\n",
      "          8.8180e-01, 7.7998e-01, 9.7386e-01],\n",
      "         [8.8553e-02, 5.1739e-01, 2.2371e-01, 6.5381e-01, 9.7929e-01,\n",
      "          1.5514e-01, 2.1258e-01, 8.2729e-01, 8.0688e-01, 2.8087e-01,\n",
      "          1.3985e-01, 8.6918e-01, 3.0360e-01, 7.9370e-02, 3.1813e-02,\n",
      "          7.0106e-01, 1.9563e-01, 3.6173e-01, 1.4721e-02, 5.1982e-01,\n",
      "          9.8866e-01, 5.9671e-01, 4.2284e-01, 4.0466e-01, 2.7220e-01,\n",
      "          9.6252e-01, 2.6233e-01, 6.4557e-01],\n",
      "         [3.9707e-01, 6.8570e-01, 4.6453e-01, 6.8225e-01, 6.5385e-01,\n",
      "          8.1035e-01, 5.9425e-01, 2.4206e-02, 6.9655e-01, 9.9398e-01,\n",
      "          2.3974e-01, 6.3934e-01, 7.2328e-01, 9.2470e-01, 2.3636e-01,\n",
      "          9.1461e-01, 8.1522e-01, 7.4504e-01, 6.5309e-01, 3.6571e-02,\n",
      "          1.7309e-01, 2.1205e-01, 5.2115e-02, 1.7806e-01, 6.8876e-01,\n",
      "          9.2203e-01, 3.0378e-01, 1.9839e-01],\n",
      "         [4.8354e-01, 4.7542e-01, 3.8072e-01, 9.9582e-01, 8.6288e-02,\n",
      "          5.8284e-01, 2.5108e-01, 1.7005e-01, 6.1999e-01, 4.1354e-01,\n",
      "          9.7386e-01, 1.7063e-02, 7.6460e-01, 3.7311e-01, 1.4368e-01,\n",
      "          1.5037e-02, 8.3011e-01, 9.9262e-01, 4.3656e-01, 8.9482e-01,\n",
      "          6.2281e-02, 9.5545e-01, 9.9736e-01, 7.9208e-01, 5.3452e-01,\n",
      "          9.7217e-01, 1.7623e-01, 3.4943e-01],\n",
      "         [7.8861e-01, 9.1626e-02, 2.5274e-01, 4.9198e-02, 7.6288e-01,\n",
      "          1.1789e-01, 5.7545e-01, 3.1461e-01, 8.5465e-01, 9.5573e-02,\n",
      "          5.2403e-01, 5.0006e-01, 2.0239e-01, 6.7387e-01, 9.3718e-01,\n",
      "          6.7536e-02, 5.0162e-01, 4.5615e-01, 2.0827e-01, 9.0181e-01,\n",
      "          9.7060e-01, 6.2020e-01, 5.9344e-01, 9.0301e-01, 2.0461e-01,\n",
      "          6.0887e-01, 8.7877e-01, 3.6656e-01],\n",
      "         [5.4947e-01, 7.4966e-01, 8.5450e-01, 2.7164e-01, 9.8665e-01,\n",
      "          8.4394e-01, 6.9310e-02, 2.8759e-01, 4.0468e-01, 1.9803e-01,\n",
      "          7.0814e-01, 4.1025e-01, 1.7195e-01, 1.9223e-01, 2.4223e-01,\n",
      "          6.1583e-01, 4.3729e-01, 3.4644e-01, 8.7108e-01, 7.8974e-01,\n",
      "          5.9836e-01, 9.0456e-01, 1.8659e-01, 8.5607e-01, 9.7973e-01,\n",
      "          9.5816e-01, 7.0250e-02, 3.0749e-01],\n",
      "         [2.2357e-01, 3.4343e-02, 6.4601e-01, 7.8154e-01, 9.0515e-01,\n",
      "          6.7911e-01, 7.0864e-01, 6.8300e-01, 1.8252e-02, 5.7228e-01,\n",
      "          2.0849e-01, 6.6856e-01, 5.5199e-01, 4.6716e-01, 7.4191e-01,\n",
      "          8.9911e-01, 1.5917e-01, 7.2031e-03, 3.7773e-01, 5.9956e-02,\n",
      "          2.2497e-01, 6.8888e-01, 3.8110e-01, 7.0511e-01, 9.5199e-01,\n",
      "          5.9855e-01, 6.8301e-01, 9.6609e-01],\n",
      "         [5.7699e-01, 8.6912e-01, 1.4023e-01, 5.8914e-01, 6.0881e-01,\n",
      "          6.3269e-01, 3.9079e-01, 7.9620e-01, 1.6895e-02, 7.1054e-01,\n",
      "          5.0311e-01, 4.3331e-01, 6.0152e-02, 4.8128e-01, 3.1804e-01,\n",
      "          2.5461e-01, 5.3580e-01, 6.0782e-01, 4.6249e-01, 2.4858e-01,\n",
      "          9.8096e-01, 5.8447e-02, 7.7012e-01, 1.1829e-01, 3.6270e-01,\n",
      "          1.3759e-01, 6.7234e-01, 7.8243e-02],\n",
      "         [9.6245e-02, 6.9498e-01, 1.8122e-01, 6.5881e-01, 7.6555e-01,\n",
      "          9.3035e-01, 5.7809e-02, 8.3057e-01, 4.3190e-01, 6.2269e-01,\n",
      "          1.6941e-01, 2.8884e-01, 1.4992e-01, 4.8123e-01, 5.3843e-01,\n",
      "          2.5716e-02, 4.7335e-01, 4.5654e-01, 9.8295e-01, 6.3629e-01,\n",
      "          8.2328e-01, 3.7142e-01, 3.8115e-01, 5.7134e-01, 2.6792e-01,\n",
      "          8.3565e-01, 8.6379e-01, 5.4734e-01],\n",
      "         [4.5329e-01, 1.7361e-01, 6.6581e-01, 6.0981e-01, 9.1039e-01,\n",
      "          3.8586e-01, 8.0786e-01, 2.0962e-01, 9.7388e-01, 9.6046e-01,\n",
      "          6.4246e-01, 2.0807e-01, 2.9346e-01, 8.5644e-01, 2.9693e-01,\n",
      "          4.9932e-01, 7.0516e-01, 3.2694e-01, 5.7091e-01, 6.7918e-01,\n",
      "          8.4361e-01, 7.6827e-02, 3.1471e-01, 1.5194e-01, 7.7586e-01,\n",
      "          8.6590e-01, 9.5843e-01, 4.9523e-01],\n",
      "         [4.5477e-01, 3.7605e-02, 1.8839e-01, 9.2436e-01, 6.9783e-01,\n",
      "          8.8662e-02, 7.1114e-01, 3.5411e-01, 9.9657e-01, 1.1764e-01,\n",
      "          6.1707e-01, 9.2686e-01, 6.2491e-02, 9.0142e-01, 2.9805e-02,\n",
      "          8.1954e-01, 8.0095e-01, 2.2990e-02, 3.5003e-01, 3.2380e-01,\n",
      "          6.5249e-01, 4.0131e-01, 1.0499e-01, 7.4990e-01, 7.3332e-01,\n",
      "          3.2339e-01, 5.4795e-01, 2.2580e-02],\n",
      "         [6.0956e-01, 7.3884e-01, 6.2688e-01, 6.9384e-01, 2.0714e-02,\n",
      "          2.9343e-01, 6.5714e-02, 1.4271e-01, 8.2412e-01, 7.7078e-02,\n",
      "          7.0373e-03, 3.3457e-01, 2.9111e-01, 7.9770e-01, 5.1321e-01,\n",
      "          3.9788e-01, 4.1322e-02, 1.5062e-01, 5.0469e-01, 8.0633e-02,\n",
      "          6.8630e-01, 4.8618e-01, 1.8054e-01, 1.1727e-01, 8.5515e-01,\n",
      "          1.9609e-01, 6.1306e-01, 4.6750e-01],\n",
      "         [5.4807e-01, 4.3578e-01, 1.2799e-01, 9.9665e-02, 6.8236e-01,\n",
      "          7.9209e-01, 2.4558e-01, 3.3832e-02, 1.7948e-02, 9.2258e-01,\n",
      "          6.1952e-01, 3.8228e-01, 3.9159e-01, 2.6820e-01, 3.6618e-01,\n",
      "          2.8259e-01, 6.5920e-01, 1.1513e-02, 4.5084e-02, 8.1853e-01,\n",
      "          9.1522e-01, 4.3120e-02, 8.8753e-01, 3.3984e-01, 9.1188e-01,\n",
      "          5.4505e-01, 8.4929e-01, 1.0139e-01],\n",
      "         [6.5842e-01, 3.4471e-01, 9.4099e-02, 2.0747e-01, 3.1552e-01,\n",
      "          4.3198e-01, 5.9187e-01, 6.6622e-02, 8.1794e-01, 1.5983e-01,\n",
      "          6.8347e-01, 9.4378e-01, 7.4123e-01, 9.7046e-01, 2.6097e-01,\n",
      "          7.7916e-01, 8.9601e-01, 5.7928e-01, 9.8118e-01, 5.3281e-01,\n",
      "          3.8561e-01, 9.5538e-01, 2.5850e-01, 7.5949e-01, 5.2546e-01,\n",
      "          6.3224e-01, 7.4042e-01, 5.1876e-01],\n",
      "         [3.7240e-01, 8.4763e-01, 6.1911e-01, 1.6497e-01, 1.1523e-02,\n",
      "          5.4670e-02, 2.2179e-01, 1.2281e-01, 7.2204e-01, 3.5890e-01,\n",
      "          2.8402e-01, 2.3258e-01, 1.4693e-01, 5.6840e-03, 3.8344e-01,\n",
      "          4.8932e-01, 4.1282e-01, 1.5979e-01, 8.0109e-01, 2.6152e-01,\n",
      "          3.9395e-01, 4.0473e-01, 4.3976e-01, 5.6539e-01, 5.1966e-01,\n",
      "          8.7601e-01, 2.5478e-01, 5.4554e-01],\n",
      "         [1.6949e-01, 2.2178e-01, 4.3236e-01, 2.2186e-01, 7.6703e-01,\n",
      "          1.5468e-01, 3.1971e-02, 4.6681e-01, 3.5924e-02, 7.0036e-01,\n",
      "          4.2952e-01, 8.7044e-01, 9.6407e-01, 4.3099e-01, 5.6918e-01,\n",
      "          9.2866e-01, 4.5400e-01, 9.5242e-01, 9.8388e-01, 9.1856e-01,\n",
      "          5.3714e-01, 4.5319e-01, 8.3470e-01, 6.5605e-01, 7.9180e-01,\n",
      "          7.0589e-01, 8.2567e-03, 1.8943e-01],\n",
      "         [3.0830e-01, 3.8138e-01, 5.3755e-01, 6.1414e-01, 8.7935e-01,\n",
      "          2.2952e-01, 9.2065e-01, 5.4924e-01, 3.1133e-01, 9.1362e-04,\n",
      "          8.8624e-01, 7.9384e-01, 8.4688e-01, 6.1955e-01, 1.1393e-01,\n",
      "          8.7034e-01, 4.7686e-01, 7.2565e-01, 8.0670e-01, 1.8392e-01,\n",
      "          3.6073e-01, 1.3148e-01, 1.0806e-02, 5.7791e-01, 4.8687e-01,\n",
      "          7.7526e-01, 5.0709e-01, 7.0168e-02],\n",
      "         [6.6210e-01, 1.6396e-01, 5.3940e-01, 5.2094e-01, 6.6908e-01,\n",
      "          3.0229e-01, 3.9008e-01, 5.9955e-01, 2.4045e-01, 1.4734e-02,\n",
      "          9.3220e-01, 3.8728e-01, 2.3909e-01, 5.7932e-01, 2.2824e-01,\n",
      "          5.5585e-01, 3.8382e-01, 9.2187e-01, 7.3107e-01, 7.5250e-01,\n",
      "          5.1044e-01, 5.9127e-01, 4.8264e-01, 9.7599e-02, 2.8926e-04,\n",
      "          6.4273e-02, 9.0760e-01, 2.9524e-01],\n",
      "         [3.2728e-03, 2.9672e-01, 9.1989e-01, 9.9186e-01, 8.5382e-01,\n",
      "          2.3886e-01, 2.2211e-01, 1.7265e-01, 2.4631e-01, 1.1387e-01,\n",
      "          5.2547e-01, 5.4792e-02, 9.9127e-01, 2.7184e-01, 7.6738e-01,\n",
      "          5.8052e-02, 1.9073e-02, 6.8880e-01, 8.8526e-01, 2.5485e-01,\n",
      "          4.0776e-02, 9.3420e-01, 9.2003e-01, 7.9762e-01, 6.5084e-01,\n",
      "          7.9030e-01, 2.5148e-01, 2.9604e-01],\n",
      "         [5.9361e-01, 3.0600e-01, 3.5127e-01, 4.6267e-01, 7.8602e-01,\n",
      "          9.3212e-01, 4.8973e-01, 2.4186e-01, 6.3330e-01, 5.0212e-01,\n",
      "          1.5042e-01, 3.5722e-01, 8.7672e-01, 2.5969e-01, 3.1182e-01,\n",
      "          4.8903e-01, 8.8118e-01, 6.2870e-02, 6.8335e-01, 3.4462e-01,\n",
      "          4.8593e-01, 6.8957e-03, 6.5961e-01, 8.8798e-01, 2.1294e-01,\n",
      "          5.0456e-01, 4.4245e-01, 8.0012e-01],\n",
      "         [6.1901e-01, 6.0905e-01, 3.5648e-01, 2.9915e-01, 4.7761e-01,\n",
      "          3.1118e-01, 9.0563e-01, 2.0590e-01, 8.0524e-01, 6.7695e-01,\n",
      "          3.2761e-01, 6.4754e-01, 9.4177e-01, 6.8438e-01, 4.2761e-02,\n",
      "          7.4277e-01, 3.4406e-01, 5.8317e-01, 4.3927e-01, 9.8164e-01,\n",
      "          9.1217e-01, 4.7359e-01, 2.0132e-01, 1.0530e-01, 2.3084e-01,\n",
      "          5.9712e-01, 8.1915e-01, 1.1592e-02]]])\n",
      "Predicted class: 4\n",
      "logits: tensor([[-0.0279, -0.0373, -0.0142,  0.0902,  0.1036, -0.0049, -0.0387,  0.0520,\n",
      "          0.0693, -0.0278]], grad_fn=<AddmmBackward0>)\n",
      "pred_probab:tensor([[0.0955, 0.0946, 0.0968, 0.1075, 0.1090, 0.0977, 0.0945, 0.1035, 0.1053,\n",
      "         0.0955]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"input:{X}\")\n",
    "print(f\"Predicted class: {y_pred.item()}\")\n",
    "print(f\"logits: {logits}\")\n",
    "print(f\"pred_probab:{pred_probab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cd286-63ee-476a-a3ce-e4d7365f5a47",
   "metadata": {},
   "source": [
    "28 inputs, each 28 values long.\n",
    "\n",
    "<br/>\n",
    "\n",
    "The output is 10 values long logits.\n",
    "\n",
    "<br/>\n",
    "\n",
    "the highest softmax(logits) probablity is the one that was returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5020c-a901-446d-a06f-3cf74ce6a4aa",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "\n",
    "Let’s break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a6c67d0-1591-49c1-93bc-984caafb6748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "for i in range(0,len(input_image)):\n",
    "    img = input_image[0].squeeze()\n",
    "\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071163a-cb44-4611-8f94-3493e7bdcc9a",
   "metadata": {},
   "source": [
    "## nn.Flatten\n",
    "\n",
    "We initialize the nn.Flatten layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8503ed79-2a6c-4dfe-9a02-b177e7d5f972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522e2b0-7592-4a9a-8107-0ef2cdedf28c",
   "metadata": {},
   "source": [
    "## nn.Sequential\n",
    "\n",
    "nn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a36192c5-f5c0-4343-a727-ca7988e99159",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c26ef-e428-4ab1-b027-caf28cd86293",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "\n",
    "The linear layer is a module that applies a linear transformation on the input using its stored weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f768692-3788-4042-9b08-e7ca51f1f8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8be9f-4544-41dc-8825-bdebbc9dfc77",
   "metadata": {},
   "source": [
    "### nn.ReLU\n",
    "\n",
    "Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "\n",
    "In this model, we use nn.ReLU between our linear layers, but there’s other activations to introduce non-linearity in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3cf27c32-b5de-4b3c-b6fb-ec4ac15d6c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.4390,  0.2300, -0.3363, -0.9431,  0.0906, -0.1378,  0.2716,  0.0387,\n",
      "         -0.4681,  0.0927,  0.6127, -0.0531, -0.2481,  0.2586, -0.0270,  0.5453,\n",
      "         -0.2401,  0.0093, -0.1789, -0.0947],\n",
      "        [ 0.6639,  0.2188, -0.2369, -0.8736,  0.0778,  0.1491,  0.2682, -0.0215,\n",
      "         -0.7511, -0.2793,  0.2325, -0.3523, -0.4271,  0.0523, -0.1073,  0.5265,\n",
      "         -0.0663, -0.3978, -0.0149, -0.0351],\n",
      "        [ 0.4945,  0.1803,  0.0359, -0.9275, -0.0472,  0.0437,  0.2173,  0.2847,\n",
      "         -0.5555,  0.1718,  0.5920, -0.2198, -0.1712,  0.0529,  0.0108,  0.5161,\n",
      "          0.0592, -0.0694, -0.0887, -0.2098]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.4390, 0.2300, 0.0000, 0.0000, 0.0906, 0.0000, 0.2716, 0.0387, 0.0000,\n",
      "         0.0927, 0.6127, 0.0000, 0.0000, 0.2586, 0.0000, 0.5453, 0.0000, 0.0093,\n",
      "         0.0000, 0.0000],\n",
      "        [0.6639, 0.2188, 0.0000, 0.0000, 0.0778, 0.1491, 0.2682, 0.0000, 0.0000,\n",
      "         0.0000, 0.2325, 0.0000, 0.0000, 0.0523, 0.0000, 0.5265, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.4945, 0.1803, 0.0359, 0.0000, 0.0000, 0.0437, 0.2173, 0.2847, 0.0000,\n",
      "         0.1718, 0.5920, 0.0000, 0.0000, 0.0529, 0.0108, 0.5161, 0.0592, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7dc5c-ae79-4e35-8f95-18bf93d3a9e4",
   "metadata": {},
   "source": [
    "Above all, this transformation simplifies all returns as either a probabilty or a non-occurence, so to speak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263ba9d-a616-4572-a8af-e78298138c3b",
   "metadata": {},
   "source": [
    "### nn.Softmax\n",
    "\n",
    "The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48728af2-f8f3-4c77-b76c-b52bd62a4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff20c2de-6d1a-4aeb-93e4-d4f01b48811a",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "\n",
    "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43043efc-6bfe-489c-94ed-0d4044959715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0088,  0.0003,  0.0353,  ..., -0.0226,  0.0101,  0.0076],\n",
      "        [-0.0183,  0.0080,  0.0146,  ..., -0.0334, -0.0332,  0.0089]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0024, -0.0148], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0377, -0.0029,  0.0300,  ...,  0.0188,  0.0129,  0.0017],\n",
      "        [-0.0196,  0.0192,  0.0137,  ..., -0.0157, -0.0270,  0.0048]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0377, -0.0427], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0338,  0.0376, -0.0333,  ...,  0.0094,  0.0442, -0.0167],\n",
      "        [-0.0368, -0.0182,  0.0193,  ...,  0.0227,  0.0260,  0.0403]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0377, -0.0231], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b8255-20fc-41ff-981e-9f751ebd70f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241dcf2-6915-4067-8d08-a0540fd88b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dca8a0-22b3-4767-a8cc-dcb6ade6f1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66cf11-6836-4942-86d5-7c4c4a86611d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c28b1-b733-45bb-8e99-6a4c9d7921de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcc448-b2f1-428e-8fbf-f05847d0ade6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d9b79c-71a8-4633-beb2-58059865a09b",
   "metadata": {},
   "source": [
    "GANs are composed of two models trained in unison. The first model, the generator, takes in some random input and tries to output something that looks like our training data. The second model, the discriminator, takes in training data and generated data and tries to distinguish the fake generated data from the real training data. What makes this framework interesting is that these models are trained together. As the discriminator gets better at recognizing fake images this learning is passed to the generator and the generator gets better at generating fake images. To overuse an analogy the generator is to the discriminator as a counterfeiter is to FBI investigators. One tries to forge data, the other tries to distinguish forgeries from the real deal.\n",
    "\n",
    "This framework has produced a ton of super interesting results in the last few years from translating horses to zebras, to creating deep fakes, to imagining up wholly new images. In this tutorial we aren’t going to do anything as interesting as those but this should give you all of the background you need in order to successfully implement a GAN of your own from scratch : ). Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df3be4-957d-4006-843b-2dc78667c092",
   "metadata": {},
   "source": [
    "Problem Definition\n",
    "\n",
    "Imagine that we have a data set of all even numbers between zero and 128. This is a subset of a much bigger distribution of data, the integers, with some specific properties, much like how human faces are a subset of all images of living things. Our generator is going to take in random noise as an integer in that same range and learn to produce only even numbers.\n",
    "\n",
    "Before getting into the actual model let’s build out our data set. We are going to represent each integer as it’s unsigned seven bit binary representation. So the number 56 is 0111000. We do this because:\n",
    "\n",
    "    It is very natural to pass in a binary vector to a machine learning algorithm, in this case, a neural network.\n",
    "    It is easy to see if the model is generating even numbers by looking at the lowest bit. If it’s a one the number is odd, if it’s a zero the number is even.\n",
    "\n",
    "To start let’s write a function which converts any positive integer to its binary form as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa3ff16c-68c6-4750-b98d-62c4923e3d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def create_binary_list_from_int(number = int):\n",
    "    if number < 0 or type(number) is not int:\n",
    "        raise ValueError(\"Only Positive integers are allowed\")\n",
    "\n",
    "    return [int(x) for x in list(bin(number))[2:]]\n",
    "\n",
    "print(create_binary_list_from_int(99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53f481-37c2-4b55-8559-3615a581e555",
   "metadata": {},
   "source": [
    "With this we can make a function which will generate random training data for us on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f64721a-a839-4821-8cc0-b6617247e5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function generate_even_data at 0x7fab4ed130d0>\n"
     ]
    }
   ],
   "source": [
    "def generate_even_data(max_int: int, batch_size: int=16) -> tuple[List[int], List[List[int]]]:\n",
    "    # Get the number of binary places needed to represent the maximum number\n",
    "    max_length = int(math.log(max_int, 2))\n",
    "\n",
    "    # Sample batch_size number of integers in range 0-max_int\n",
    "    sampled_integers = np.random.randint(0, int(max_int / 2), batch_size)\n",
    "\n",
    "    # create a list of labels all ones because all numbers are even\n",
    "    labels = [1] * batch_size\n",
    "\n",
    "    # Generate a list of binary numbers for training.\n",
    "    data = [create_binary_list_from_int(int(x * 2)) for x in sampled_integers]\n",
    "    data = [([0] * (max_length - len(x))) + x for x in data]\n",
    "\n",
    "    return labels, data\n",
    "print(generate_even_data\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8595d4-110d-4a71-b020-f62152620c0a",
   "metadata": {},
   "source": [
    "This function will produce two outputs the first is a list of ones representing that this data is even and comes from our true distribution. The second output is a random even number in binary list form. That’s all we need to start building and training our models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f4b08-5aed-477f-9e48-1330726f5055",
   "metadata": {},
   "source": [
    "## Building the Generator and Discriminator Generator\n",
    "\n",
    "Building the Generator and Discriminator is a snap! Let’s start with the Generator. We need something capable of mapping random seven digit binary input to seven digit binary input that is even. The simplest possible thing here is a single seven neuron layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f40f0e90-fb77-4f96-81a0-f0b0dfe64e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40f51b4a-1856-4261-8405-b9e79c1b2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense_layer = nn.Linear(int(input_length), int(input_length))\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dense_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264da8da-4a53-4fef-9256-00953629d38a",
   "metadata": {},
   "source": [
    "If we were building a GAN to do something more complicated on say images we would probably train it using random noise generated from a normal distribution and gradually upsample and reshape it until it’s the same size as the data we are trying to copy. Since our example is so simple, a single linear layer with a logistic (sigmoid) activation should be enough to map ones and zeros in seven positions to other ones and zeros in seven positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7427a65-c430-4f54-b272-61eaaad9573b",
   "metadata": {},
   "source": [
    "Discriminator\n",
    "\n",
    "The Discriminator is no more complicated than the Generator. Here we need a model to take in a seven digit binary input and output whether or not it is from our real data distribution (is even) or not (is odd or not a number). To accomplish this we use a single neuron model (logistic regression) with a logistic activation (Sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b14b2d-bdc0-499e-9b7d-68ba9b954f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense = nn.Linear(int(input_length), 1);\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dense(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27f65e-f110-46a9-bbb5-e46d278d65cb",
   "metadata": {},
   "source": [
    "That’s it, we’ve built the two models which we will train in unison. Now for the tricky part of GAN training, the training. We need to link these models up in a way that can propagate the gradients around correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f650402-769b-46cf-b201-043d2a95415d",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "\n",
    "Training GANs can seem a bit confusing at first because we need to update two models with every bit of input and we need to be careful about how we do that. So to break it down, we pass two batches of data to our model at every training step. One batch is random noise which will cause the generator to create some generated data, and the second batch is composed solely of data from our true distribution. Throughout the training description, I will reference line numbers in the final training code gist below, not the Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8072e34-a025-4094-9570-b42232b1d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train(max_int: int = 128, batch_size: int = 16, training_steps: int = 500):\n",
    "    input_length = int(math.log(max_int, 2))\n",
    "\n",
    "    # Models\n",
    "    generator = Generator(input_length)\n",
    "    discriminator = Discriminator(input_length)\n",
    "\n",
    "    # Optimizers\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "    # loss\n",
    "    loss = nn.BCELoss()\n",
    "\n",
    "    for i in range(training_steps):\n",
    "        # zero the gradients on each iteration\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        # Create noisy input for generator\n",
    "        # Need float type instead of int\n",
    "        noise = torch.randint(0, 2, size=(batch_size, input_length)).float()\n",
    "        generated_data = generator(noise)\n",
    "\n",
    "        # Generate examples of even real data\n",
    "        true_labels, true_data = generate_even_data(max_int, batch_size=batch_size)\n",
    "        true_labels = torch.tensor(true_labels).float()\n",
    "        true_data = torch.tensor(true_data).float()\n",
    "\n",
    "        # Train the generator\n",
    "        # We invert the labels here and don't train the discriminator because we want the generator\n",
    "        # to make things the discriminator classifies as true.\n",
    "        generator_discriminator_out = discriminator(generated_data)\n",
    "        generator_loss = loss(generator_discriminator_out, true_labels)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Train the discriminator on the true/generated data\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        true_discriminator_out = discriminator(true_data)\n",
    "        true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "        # add .detach() here think about this\n",
    "        generator_discriminator_out = discriminator(generated_data.detach())\n",
    "        generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
    "        discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528382c-2814-4799-8a53-5036dcdfde29",
   "metadata": {},
   "source": [
    "Train the Generator\n",
    "\n",
    "Let’s start by training the generator. This consists of:\n",
    "\n",
    "    Creating random noise. (Line 27)\n",
    "    Generating new “fake” data by passing the noise to the generator (Line 28)\n",
    "    Get the predictions from the discriminator on the “fake” data (Line 38)\n",
    "    Calculate the loss from the discriminator’s output using labels as if the data were “real” instead of fake. (Line 39)\n",
    "    Backpropagate the error through just the generator. (Lines 40–41)\n",
    "\n",
    "Notice how in step four we use true labels instead of fake labels for calculating the loss. This is because we are training the generator. The generator should be trying to fool the discriminator so when the discriminator makes a mistake and says the generated output is real (predicts 1) then the gradients should be small, when the discriminator acts correctly and predicts that the output is generated (predicts 0) the gradients should be big. This is why we only propagate the gradients through the generator at this step, because we inverted the labels. If we trained the entire model like this either the generator would learn the wrong thing or the discriminator would.\n",
    "Train the Discriminator\n",
    "\n",
    "Now it’s time to update the weights in our discriminator. We do that in a few steps:\n",
    "\n",
    "    Pass in a batch of only data from the true data set with a vector of all one labels. (Lines 44–46)\n",
    "    Pass our generated data into the discriminator, with detached weights, and zero labels. (Lines 49–50)\n",
    "    Average the loss from steps one and two. (Line 51)\n",
    "    Backpropagate the gradients through just the discriminator. (Lines 52–53)\n",
    "\n",
    "The discriminator is trying to learn to distinguish real data from “fake” generated data. The labels while training the discriminator need to represent that, i.e. one when our data comes from the real data set and zero when it is generated by our generator. We pass in those two batches in steps (1) and (2) above and then average the loss from the two batches. It’s important to note that when passing in the generated data we want to detach the gradients. We do this because we are not training the generator we are just focused on the discriminator. Once all of that is done we backpropagate the gradients in only the discriminator and we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62878ea-91a1-4852-9df4-38aee5d31032",
   "metadata": {},
   "source": [
    "Wrapping Up\n",
    "\n",
    "That’s it! We’ve built our entire GAN. Wrap that in a training loop with some gradient zeroing at each step and we’re ready to roll. If we look at the output of our generator at various training steps we can see it converging to only creating even numbers which is exactly what we wanted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf35b1-855a-4681-b929-3c4c6e689173",
   "metadata": {},
   "source": [
    "0   : [47, 3, 35, 1, 16, 56, 39, 16, 3, 1]\n",
    "50  : [2, 35, 34, 34, 38, 2, 34, 43, 3, 43]\n",
    "100 : [42, 43, 106, 38, 35, 42, 35, 42, 43, 106]\n",
    "200 : [108, 106, 106, 42, 106, 42, 106, 106, 42, 96]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2fc0a-038b-4a86-b93e-64257cbedfa4",
   "metadata": {},
   "source": [
    "At step zero we have 7/10 odd numbers in our sample and at step 200 10/10 of our samples are even numbers! That’s a successful generator and it only took ~50 lines of real Python code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e029eef-09e7-4df7-a8b6-20c05616c177",
   "metadata": {},
   "source": [
    "What Next?\n",
    "\n",
    "As you’ve probably guessed there are some other tricks for training a GAN which generates non trivial output. Some immediate things to try if you want to make this model work on real data like images are:\n",
    "\n",
    "    The Generator will probably need to be a bit deeper and scale up the noise to the size of the real data. You can do this using transposed convolutions or upsampling layers.\n",
    "    Change the noise input to the generator to be Gaussian\n",
    "    Increase the depth of the discriminator so that its capacity for prediction is better.\n",
    "    Train for much much longer and monitor the loss.\n",
    "\n",
    "As a good next step try and implement the DCGAN architecture. This code will get you 90% of the way there. Once you’ve done that and made some fun images like those in the introduction, try and improve them by playing around with training hyper parameters. A good list of things to try when training real GANs can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9aac3-2a3a-4df2-a7a2-bb47f3fab570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
